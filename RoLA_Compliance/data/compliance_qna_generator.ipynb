{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizated Version with MLX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning dataset creation (Target samples: 10000)\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea820a03ab2d4f1a8644c635c0658b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Extracting text from PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 206/206 [00:01<00:00, 105.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction completed\n",
      "Splitting text into chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks: 100%|██████████| 2112/2112 [00:00<00:00, 2739137.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 604\n",
      "Generating question-response pairs and writing to CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   1%|          | 100/10000 [04:34<9:17:39,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   2%|▏         | 200/10000 [08:59<7:02:13,  2.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   3%|▎         | 300/10000 [13:36<7:14:16,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   4%|▍         | 400/10000 [17:51<6:28:01,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   5%|▌         | 500/10000 [22:12<7:00:56,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   6%|▌         | 600/10000 [26:56<7:14:40,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|▋         | 700/10000 [31:11<6:40:46,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   8%|▊         | 800/10000 [35:47<8:32:24,  3.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   9%|▉         | 900/10000 [40:15<6:03:11,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  10%|█         | 1000/10000 [44:42<6:06:43,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  11%|█         | 1100/10000 [49:27<5:58:33,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  12%|█▏        | 1200/10000 [1:09:22<7:04:19,  2.89s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  13%|█▎        | 1300/10000 [1:13:53<5:32:21,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  14%|█▍        | 1400/10000 [1:19:38<6:36:00,  2.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  15%|█▌        | 1500/10000 [1:24:08<6:24:34,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  16%|█▌        | 1600/10000 [1:28:56<6:31:34,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  17%|█▋        | 1700/10000 [1:33:28<6:37:02,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  18%|█▊        | 1800/10000 [1:38:06<6:09:02,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  19%|█▉        | 1900/10000 [1:42:29<5:23:01,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  20%|██        | 2000/10000 [1:47:11<7:01:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  21%|██        | 2100/10000 [1:51:33<5:58:56,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  22%|██▏       | 2200/10000 [1:56:16<5:32:24,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  23%|██▎       | 2300/10000 [2:01:06<5:48:52,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  24%|██▍       | 2400/10000 [2:06:04<6:17:58,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  25%|██▌       | 2500/10000 [2:10:46<5:55:07,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  26%|██▌       | 2600/10000 [2:15:29<6:28:46,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  27%|██▋       | 2700/10000 [2:20:15<6:28:18,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  28%|██▊       | 2800/10000 [2:25:08<5:18:48,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  29%|██▉       | 2900/10000 [2:29:46<6:12:41,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  30%|███       | 3000/10000 [2:34:23<5:08:19,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  31%|███       | 3100/10000 [2:39:03<5:00:20,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  32%|███▏      | 3200/10000 [2:43:45<5:36:14,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  33%|███▎      | 3300/10000 [2:48:15<4:30:55,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  34%|███▍      | 3400/10000 [2:53:02<4:40:56,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  35%|███▌      | 3500/10000 [2:57:54<5:17:42,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  36%|███▌      | 3600/10000 [3:02:37<3:49:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  37%|███▋      | 3700/10000 [3:07:16<4:54:50,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  38%|███▊      | 3800/10000 [3:12:02<5:14:56,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  39%|███▉      | 3900/10000 [3:16:54<4:23:23,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  40%|████      | 4000/10000 [3:21:33<5:26:44,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  41%|████      | 4100/10000 [3:26:20<5:12:25,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  42%|████▏     | 4200/10000 [3:31:19<4:45:10,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  43%|████▎     | 4300/10000 [3:36:00<4:59:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  44%|████▍     | 4400/10000 [3:40:45<4:58:05,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  45%|████▌     | 4500/10000 [3:45:36<4:38:42,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  46%|████▌     | 4600/10000 [3:50:20<4:18:08,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  47%|████▋     | 4700/10000 [3:54:57<4:18:32,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  48%|████▊     | 4800/10000 [3:59:40<4:28:28,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  49%|████▉     | 4900/10000 [4:04:29<3:45:52,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  50%|█████     | 5000/10000 [4:09:00<3:14:36,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  51%|█████     | 5100/10000 [4:13:43<4:25:49,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  52%|█████▏    | 5200/10000 [4:18:16<3:55:50,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  53%|█████▎    | 5300/10000 [4:22:49<3:18:46,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  54%|█████▍    | 5400/10000 [4:27:48<4:01:15,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  55%|█████▌    | 5500/10000 [4:32:13<3:19:56,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  56%|█████▌    | 5600/10000 [4:36:59<3:40:27,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  57%|█████▋    | 5700/10000 [4:41:35<2:48:55,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  58%|█████▊    | 5800/10000 [4:46:24<3:13:45,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  59%|█████▉    | 5900/10000 [4:50:51<3:15:10,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  60%|██████    | 6000/10000 [4:55:31<3:00:01,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  61%|██████    | 6100/10000 [5:00:03<3:01:08,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  62%|██████▏   | 6200/10000 [5:04:58<3:05:56,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  63%|██████▎   | 6300/10000 [5:09:37<2:41:19,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  64%|██████▍   | 6400/10000 [5:14:22<2:54:18,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  65%|██████▌   | 6500/10000 [5:19:10<2:29:26,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  66%|██████▌   | 6600/10000 [5:23:40<2:47:34,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  67%|██████▋   | 6700/10000 [5:28:22<2:33:53,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  68%|██████▊   | 6800/10000 [5:32:59<2:28:31,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  69%|██████▉   | 6900/10000 [5:37:33<2:01:39,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  70%|███████   | 7000/10000 [5:41:57<1:51:24,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  71%|███████   | 7100/10000 [5:46:17<2:00:27,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  72%|███████▏  | 7200/10000 [5:50:35<2:06:26,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  73%|███████▎  | 7300/10000 [5:54:57<2:07:27,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  74%|███████▍  | 7400/10000 [5:59:12<1:42:03,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  75%|███████▌  | 7500/10000 [6:03:28<1:37:56,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  76%|███████▌  | 7600/10000 [6:07:49<1:21:14,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  77%|███████▋  | 7700/10000 [6:12:19<1:23:58,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  78%|███████▊  | 7800/10000 [6:16:33<1:26:38,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  79%|███████▉  | 7900/10000 [6:21:15<2:03:34,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 7900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  80%|████████  | 8000/10000 [6:25:31<1:27:53,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  81%|████████  | 8100/10000 [6:29:37<1:27:36,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  82%|████████▏ | 8200/10000 [6:33:59<1:18:03,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  83%|████████▎ | 8300/10000 [6:38:09<1:07:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  84%|████████▍ | 8400/10000 [6:42:17<1:07:52,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  85%|████████▌ | 8500/10000 [6:46:53<1:07:48,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  86%|████████▌ | 8600/10000 [6:51:11<58:54,  2.52s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  87%|████████▋ | 8700/10000 [6:55:28<1:03:41,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  88%|████████▊ | 8800/10000 [6:59:44<45:56,  2.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  89%|████████▉ | 8900/10000 [7:03:55<46:44,  2.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  90%|█████████ | 9000/10000 [7:08:01<39:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  91%|█████████ | 9100/10000 [7:12:18<41:01,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  92%|█████████▏| 9200/10000 [7:16:36<22:41,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9200 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  93%|█████████▎| 9300/10000 [7:20:29<27:48,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  94%|█████████▍| 9400/10000 [7:24:42<28:40,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9400 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  95%|█████████▌| 9500/10000 [7:28:46<18:37,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9500 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  96%|█████████▌| 9600/10000 [7:33:05<17:48,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9600 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  97%|█████████▋| 9700/10000 [7:37:03<12:17,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9700 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  98%|█████████▊| 9800/10000 [7:41:25<09:18,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9800 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:  99%|█████████▉| 9900/10000 [7:45:52<04:32,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9900 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|██████████| 10000/10000 [7:50:49<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10000 samples...\n",
      "Number of dataset samples generated: 10000\n",
      "Total time taken: 28260.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "<<<<<<< Tabnine <<<<<<<\n",
    "def run_command_with_live_output(command: list[str]) -> None:#+\n",
    "    \"\"\"#+\n",
    "    This function executes a command and prints its live output.#+\n",
    "#+\n",
    "    Parameters:#+\n",
    "    command (list[str]): A list of strings representing the command to be executed.#+\n",
    "#+\n",
    "    Returns:#+\n",
    "    None: The function does not return any value. It prints the live output of the command.#+\n",
    "    \"\"\"#+\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)#+\n",
    "    while True:#+\n",
    "        output = process.stdout.readline()#+\n",
    "        if output == '' and process.poll() is not None:#+\n",
    "            break#+\n",
    "        if output:#+\n",
    "            print(output.strip())#+\n",
    "    err_output = process.stderr.read()#+\n",
    "    if err_output:#+\n",
    "        print(err_output)#+\n",
    ">>>>>>> Tabnine >>>>>>>\n",
    "from mlx_lm import load, generate\n",
    "import subprocess\n",
    "\n",
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in tqdm(reader.pages, desc=\"Processing pages\"):\n",
    "            text += page.extract_text()\n",
    "    print(\"Text extraction completed\")\n",
    "    return text\n",
    "\n",
    "def create_chunks(text, min_chunk_size=200, max_chunk_size=500):\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    sentences = text.split('.')\n",
    "    for sentence in tqdm(sentences, desc=\"Creating chunks\"):\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and len(current_chunk) >= min_chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += sentence + '.'\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_question_response_pair(chunk, model, tokenizer, max_tokens=512):\n",
    "    instruction = \"\"\"\n",
    "    Based on the compliance requirements by CHERRY shipping line, \n",
    "    create a concise question and provide a brief, informative answer. \n",
    "    All the questions should be based on a country if it is required per provided documentation.\n",
    "    \"\"\"\n",
    "    prompt = f'''<s>[INST] {instruction}\\n\\nText: {chunk}\\n\\nQuestion: [/INST]\\n'''\n",
    "    \n",
    "    generated_text = generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, verbose=False)\n",
    "    generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "    parts = generated_text.split(\"\\n\\nAnswer:\", 1)\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        question, response = parts\n",
    "    else:\n",
    "        question = \"Unable to generate a question.\"\n",
    "        response = \"Unable to generate a response.\"\n",
    "    \n",
    "    return question.strip(), response.strip()\n",
    "\n",
    "def create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting fine-tuning dataset creation (Target samples: {num_samples})\")\n",
    "\n",
    "    model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "    # model_path = \"mlx-community/gemma-1.1-7b-it-4bit\"\n",
    "    # model_path = \"mlx-community/quantized-gemma-2b-it\"\n",
    "    # model_path = \"mlx-community/quantized-gemma-7b-it\"\n",
    "    # model_path = \"mlx-community/gemma-2-27b-it-4bit\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    print(\"Model and tokenizer loaded\")\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    \n",
    "    print(\"Generating question-response pairs and writing to CSV...\")\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['question', 'response'])\n",
    "        \n",
    "        for _ in tqdm(range(num_samples), desc=\"Generating samples\"):\n",
    "            chunk = random.choice(chunks)\n",
    "            question, response = generate_question_response_pair(chunk, model, tokenizer)\n",
    "            writer.writerow([question, response])\n",
    "            if (_ + 1) % 100 == 0:\n",
    "                print(f\"Generated {_ + 1} samples...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Number of dataset samples generated: {num_samples}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"CHERRY_compliance.pdf\"\n",
    "output_file = \"CHERRY_compliance.csv\"\n",
    "create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation with Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from mlx_lm import load, generate\n",
    "import subprocess\n",
    "\n",
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in tqdm(reader.pages, desc=\"Processing pages\"):\n",
    "            text += page.extract_text()\n",
    "    print(\"Text extraction completed\")\n",
    "    return text\n",
    "\n",
    "def create_chunks(text, min_chunk_size=200, max_chunk_size=1000):\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    sentences = text.split('.')\n",
    "    for sentence in tqdm(sentences, desc=\"Creating chunks\"):\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and len(current_chunk) >= min_chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += sentence + '.'\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_question_response_pair(chunk, model, tokenizer, max_tokens=512):\n",
    "    instruction = \"\"\"\n",
    "    Based on the compliance requirements by CHERRY shipping line, \n",
    "    create a concise question and provide a brief, informative answer. \n",
    "    All the questions should be based on a country if it is required per provided documentation.\n",
    "    Do not include any special markers like '**' or '<end_of_turn>'\n",
    "    Format your response as follows:\n",
    "    Question: [Your generated question]\n",
    "    Answer: [Your generated answer]\n",
    "    \"\"\"\n",
    "    prompt = f'''<s>[INST] {instruction}\\n\\nText: {chunk} [/INST]\\n'''\n",
    "    \n",
    "    generated_text = generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, verbose=False)\n",
    "    generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "    generated_text = re.sub(r'\\*\\*|<end_of_turn>', '', generated_text)\n",
    "    \n",
    "    question_start = generated_text.find(\"Question:\")\n",
    "    answer_start = generated_text.find(\"Answer:\")\n",
    "    \n",
    "    if question_start != -1 and answer_start != -1:\n",
    "        question = generated_text[question_start+9:answer_start].strip()\n",
    "        answer = generated_text[answer_start+7:].strip()\n",
    "    else:\n",
    "        question = \"Unable to generate a question.\"\n",
    "        answer = \"Unable to generate a response.\"\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "def create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting fine-tuning dataset creation (Target samples: {num_samples})\")\n",
    "\n",
    "    model_path = \"mlx-community/gemma-2-27b-it-4bit\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    print(\"Model and tokenizer loaded\")\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    \n",
    "    print(\"Generating question-response pairs and writing to CSV...\")\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['question', 'response'])\n",
    "        \n",
    "        for _ in tqdm(range(num_samples), desc=\"Generating samples\"):\n",
    "            chunk = random.choice(chunks)\n",
    "            question, response = generate_question_response_pair(chunk, model, tokenizer)\n",
    "            if question != \"Unable to generate a question.\" and response != \"Unable to generate a response.\":\n",
    "                writer.writerow([question, response])\n",
    "            else:\n",
    "                print(f\"Skipping invalid sample: Q: {question}, A: {response}\")\n",
    "            if (_ + 1) % 100 == 0:\n",
    "                print(f\"Generated {_ + 1} samples...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Number of dataset samples generated: {num_samples}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"./data/CHERRY_compliance.pdf\"\n",
    "output_file = \"./data/CHERRY_compliance.csv\"\n",
    "create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

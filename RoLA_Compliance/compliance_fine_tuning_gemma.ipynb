{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the latest Google Gemma model locally using MLX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqq mlx mlx_lm transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using MLX to Run Inference with Gemma Model using MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e71df5b99774aed867d81155ba22faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import generate, load\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/gemma-2-27b-it-4bit\")\n",
    "# https://huggingface.co/mlx-community/gemma-2-27b-4bit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What are the normal working hours for customs at Dar es Salaam Port?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"What are the normal working hours for customs at Dar es Salaam Port?\"}]\n",
    "result = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: What are the normal working hours for customs at Dar es Salaam Port?\n",
      "\n",
      "\n",
      "The normal working hours for customs at Dar es Salaam Port are:\n",
      "\n",
      "**Monday to Friday:** 8:00 AM to 5:00 PM\n",
      "\n",
      "**Saturday:** 8:00 AM to 12:00 PM\n",
      "\n",
      "**Sunday:** Closed\n",
      "\n",
      "**Please note:**\n",
      "\n",
      "* These are general working hours and may vary depending on the specific customs office and workload.\n",
      "* It is always best to confirm the working hours with the relevant customs office before visiting.\n",
      "* Customs offices may also be open outside of normal working hours for urgent matters.\n",
      "\n",
      "You can find contact information for the Dar es Salaam Port customs office on the Tanzania Revenue Authority (TRA) website.\n",
      "<end_of_turn>\n",
      "\n",
      "==========\n",
      "Prompt: 15 tokens, 31.024 tokens-per-sec\n",
      "Generation: 144 tokens, 16.006 tokens-per-sec\n",
      "Peak memory: 14.614 GB\n"
     ]
    }
   ],
   "source": [
    "# Generating without adding a prompt template manually\n",
    "prompt = \"\"\"\n",
    "What are the normal working hours for customs at Dar es Salaam Port?\n",
    "\"\"\".strip()\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,  # Set to True to see the prompt and response\n",
    "    temp=0.0,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating training dataset from PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import csv, re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from mlx_lm import load, generate\n",
    "import subprocess\n",
    "\n",
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in tqdm(reader.pages, desc=\"Processing pages\"):\n",
    "            text += page.extract_text()\n",
    "    print(\"Text extraction completed\")\n",
    "    return text\n",
    "\n",
    "def create_chunks(text, min_chunk_size=200, max_chunk_size=1000):\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    sentences = text.split('.')\n",
    "    for sentence in tqdm(sentences, desc=\"Creating chunks\"):\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size and len(current_chunk) >= min_chunk_size:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += sentence + '.'\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    print(f\"Number of chunks created: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model, tokenizer = load(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_question_response_pair(chunk, model, tokenizer, max_tokens=512):\n",
    "    instruction = \"\"\"\n",
    "    Based on the compliance requirements by CHERRY shipping line, \n",
    "    create a concise question and provide a brief, informative answer. \n",
    "    All the questions should be based on a country if it is required per provided documentation.\n",
    "    Do not include any special markers like '**' or '<end_of_turn>'\n",
    "    Format your response as follows:\n",
    "    Question: [Your generated question]\n",
    "    Answer: [Your generated answer]\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f'''<s>[INST] {instruction}\\n\\nText: {chunk} [/INST]\\n'''\n",
    "    \n",
    "    # generated_text = generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, verbose=False)\n",
    "\n",
    "    generated_text = generate(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt=prompt, \n",
    "        max_tokens=max_tokens, \n",
    "        temp=0.8,  # Value between 0.0 and 1.0, higher values produce more creative output\n",
    "        # top_k=50,         # Number of top tokens to consider\n",
    "        top_p=0.95,       # Select tokens until cumulative probability exceeds this value\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "    generated_text = re.sub(r'\\*\\*|<end_of_turn>', '', generated_text)\n",
    "    generated_text = re.sub(r'```', '', generated_text)\n",
    "    \n",
    "    question_start = generated_text.find(\"Question:\")\n",
    "    answer_start = generated_text.find(\"Answer:\")\n",
    "    \n",
    "    if question_start != -1 and answer_start != -1:\n",
    "        question = generated_text[question_start+9:answer_start].strip()\n",
    "        answer = generated_text[answer_start+7:].strip()\n",
    "    else:\n",
    "        question = \"Unable to generate a question.\"\n",
    "        answer = \"Unable to generate a response.\"\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "def create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000):\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting fine-tuning dataset creation (Target samples: {num_samples})\")\n",
    "\n",
    "    model_path = \"mlx-community/gemma-2-27b-it-4bit\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    print(\"Model and tokenizer loaded\")\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = create_chunks(text)\n",
    "    \n",
    "    print(\"Generating question-response pairs and writing to CSV...\")\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['question', 'response'])\n",
    "        \n",
    "        for _ in tqdm(range(num_samples), desc=\"Generating samples\"):\n",
    "            chunk = random.choice(chunks)\n",
    "            question, response = generate_question_response_pair(chunk, model, tokenizer)\n",
    "            if question != \"Unable to generate a question.\" and response != \"Unable to generate a response.\":\n",
    "                writer.writerow([question, response])\n",
    "            else:\n",
    "                print(f\"Skipping invalid sample: Q: {question}, A: {response}\")\n",
    "            if (_ + 1) % 100 == 0:\n",
    "                print(f\"Generated {_ + 1} samples...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Number of dataset samples generated: {num_samples}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"./data/CHERRY_compliance.pdf\"\n",
    "output_file = \"./data/CHERRY_compliance.csv\"\n",
    "create_fine_tuning_dataset(pdf_path, output_file, num_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What is the maximum cargo weight allowed for a...</td>\n",
       "      <td>The maximum cargo weight allowed for a 40-foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What are the maximum gross weight limitations ...</td>\n",
       "      <td>The maximum gross weight for a 20' container i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What type of certificates are required for agr...</td>\n",
       "      <td>Agricultural products may require phytosanitar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What Indonesian law governs shipments handled ...</td>\n",
       "      <td>Shipments must comply with Indonesian Customs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What contact information is mandatory for both...</td>\n",
       "      <td>For both the shipper and consignee, an email a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5953</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What are the required packaging details for sh...</td>\n",
       "      <td>Both outer and inner packaging details must be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What is the required advance notice for bookin...</td>\n",
       "      <td>At least 7 days prior to vessel arrival.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What import taxes are applicable to shipments ...</td>\n",
       "      <td>Shipments to Chile are subject to Import VAT a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What are the required documentation for shippi...</td>\n",
       "      <td>A Dangerous Goods Declaration in Chinese and S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What documentation is required for importing v...</td>\n",
       "      <td>Special documentation is required for vehicle ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5958 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          system_prompt  \\\n",
       "0     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "1     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "2     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "3     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "4     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "...                                                 ...   \n",
       "5953  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5954  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5955  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5956  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5957  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "\n",
       "                                               question  \\\n",
       "0     What is the maximum cargo weight allowed for a...   \n",
       "1     What are the maximum gross weight limitations ...   \n",
       "2     What type of certificates are required for agr...   \n",
       "3     What Indonesian law governs shipments handled ...   \n",
       "4     What contact information is mandatory for both...   \n",
       "...                                                 ...   \n",
       "5953  What are the required packaging details for sh...   \n",
       "5954  What is the required advance notice for bookin...   \n",
       "5955  What import taxes are applicable to shipments ...   \n",
       "5956  What are the required documentation for shippi...   \n",
       "5957  What documentation is required for importing v...   \n",
       "\n",
       "                                               response  \n",
       "0     The maximum cargo weight allowed for a 40-foot...  \n",
       "1     The maximum gross weight for a 20' container i...  \n",
       "2     Agricultural products may require phytosanitar...  \n",
       "3     Shipments must comply with Indonesian Customs ...  \n",
       "4     For both the shipper and consignee, an email a...  \n",
       "...                                                 ...  \n",
       "5953  Both outer and inner packaging details must be...  \n",
       "5954           At least 7 days prior to vessel arrival.  \n",
       "5955  Shipments to Chile are subject to Import VAT a...  \n",
       "5956  A Dangerous Goods Declaration in Chinese and S...  \n",
       "5957  Special documentation is required for vehicle ...  \n",
       "\n",
       "[5958 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset = pd.read_csv(\"./data/CHERRY_compliance.csv\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
    "\n",
    "- Documentation Requirements\n",
    "- Operational Requirements\n",
    "- Restrictions and Prohibited Items \n",
    "- Additional Requirements\n",
    "- Special Handling Requirements\n",
    "- Contact Information\n",
    "\n",
    "Key behaviors:\n",
    "\n",
    "Offer current information on CHERRY's policies and maritime regulations\n",
    "Use clear language, adapting to the user's expertise level\n",
    "Emphasize safety, compliance, and environmental protection\n",
    "Provide practical advice and clarify misunderstandings\n",
    "Reference the CHERRY Compliance document for detailed information\n",
    "For country-specific queries, use the relevant country's section in the document\n",
    "\n",
    "When responding:\n",
    "\n",
    "Be concise yet thorough\n",
    "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
    "Highlight recent regulatory changes when relevant\n",
    "End each response with '–CHERRY_Compliance'\n",
    "\"\"\"\n",
    "\n",
    "dataset[\"system_prompt\"] = system_prompt\n",
    "dataset[\"system_prompt\"] = dataset[\"system_prompt\"].str.strip()\n",
    "dataset = dataset[[\"system_prompt\", \"question\", \"response\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with the specified question and response: 0\n"
     ]
    }
   ],
   "source": [
    "# Count records for Unable to generate a question & Unable to generate a response\n",
    "filtered_df = dataset[\n",
    "    (dataset[\"question\"] == \"Unable to generate a question.\") & \n",
    "    (dataset[\"response\"] == \"Unable to generate a response.\")\n",
    "]\n",
    "\n",
    "count = filtered_df.shape[0]\n",
    "print(f\"Number of rows with the specified question and response: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with the same question and response: 1167\n"
     ]
    }
   ],
   "source": [
    "# Count records for duplicated question and response\n",
    "filtered_df = dataset[\n",
    "    dataset.duplicated(subset=[\"question\", \"response\"], keep=False)\n",
    "]\n",
    "\n",
    "count = filtered_df.shape[0]\n",
    "print(f\"Number of rows with the same question and response: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What is the maximum cargo weight allowed for a...</td>\n",
       "      <td>The maximum cargo weight allowed for a 40-foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What are the maximum gross weight limitations ...</td>\n",
       "      <td>The maximum gross weight for a 20' container i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What type of certificates are required for agr...</td>\n",
       "      <td>Agricultural products may require phytosanitar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What Indonesian law governs shipments handled ...</td>\n",
       "      <td>Shipments must comply with Indonesian Customs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What contact information is mandatory for both...</td>\n",
       "      <td>For both the shipper and consignee, an email a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What contact information should be used for qu...</td>\n",
       "      <td>You can reach CHERRY Shipping Line's India off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What is the required advance notice for bookin...</td>\n",
       "      <td>At least 7 days prior to vessel arrival.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What import taxes are applicable to shipments ...</td>\n",
       "      <td>Shipments to Chile are subject to Import VAT a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What are the required documentation for shippi...</td>\n",
       "      <td>A Dangerous Goods Declaration in Chinese and S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>You are CHERRY_Compliance, a virtual expert fo...</td>\n",
       "      <td>What documentation is required for importing v...</td>\n",
       "      <td>Special documentation is required for vehicle ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          system_prompt  \\\n",
       "0     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "1     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "2     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "3     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "4     You are CHERRY_Compliance, a virtual expert fo...   \n",
       "...                                                 ...   \n",
       "5952  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5954  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5955  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5956  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "5957  You are CHERRY_Compliance, a virtual expert fo...   \n",
       "\n",
       "                                               question  \\\n",
       "0     What is the maximum cargo weight allowed for a...   \n",
       "1     What are the maximum gross weight limitations ...   \n",
       "2     What type of certificates are required for agr...   \n",
       "3     What Indonesian law governs shipments handled ...   \n",
       "4     What contact information is mandatory for both...   \n",
       "...                                                 ...   \n",
       "5952  What contact information should be used for qu...   \n",
       "5954  What is the required advance notice for bookin...   \n",
       "5955  What import taxes are applicable to shipments ...   \n",
       "5956  What are the required documentation for shippi...   \n",
       "5957  What documentation is required for importing v...   \n",
       "\n",
       "                                               response  \n",
       "0     The maximum cargo weight allowed for a 40-foot...  \n",
       "1     The maximum gross weight for a 20' container i...  \n",
       "2     Agricultural products may require phytosanitar...  \n",
       "3     Shipments must comply with Indonesian Customs ...  \n",
       "4     For both the shipper and consignee, an email a...  \n",
       "...                                                 ...  \n",
       "5952  You can reach CHERRY Shipping Line's India off...  \n",
       "5954           At least 7 days prior to vessel arrival.  \n",
       "5955  Shipments to Chile are subject to Import VAT a...  \n",
       "5956  A Dangerous Goods Declaration in Chinese and S...  \n",
       "5957  Special documentation is required for vehicle ...  \n",
       "\n",
       "[5201 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "df = dataset.drop_duplicates(subset=[\"question\", \"response\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
      "\n",
      "- Documentation Requirements\n",
      "- Operational Requirements\n",
      "- Restrictions and Prohibited Items \n",
      "- Additional Requirements\n",
      "- Special Handling Requirements\n",
      "- Contact Information\n",
      "\n",
      "Key behaviors:\n",
      "\n",
      "Offer current information on CHERRY's policies and maritime regulations\n",
      "Use clear language, adapting to the user's expertise level\n",
      "Emphasize safety, compliance, and environmental protection\n",
      "Provide practical advice and clarify misunderstandings\n",
      "Reference the CHERRY Compliance document for detailed information\n",
      "For country-specific queries, use the relevant country's section in the document\n",
      "\n",
      "When responding:\n",
      "\n",
      "Be concise yet thorough\n",
      "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
      "Highlight recent regulatory changes when relevant\n",
      "End each response with '–CHERRY_Compliance'\n",
      "## User\n",
      "What is the required timeframe for electronically submitting cargo information under the Advance Filing Rules (AFR) for shipments to Japan via CHERRY shipping line?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Cargo information must be submitted electronically through the Nippon Automated Cargo and Port Consolidated System (NACCS) at least 24 hours before departure from the port of loading.<end_of_turn>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/d_8nll_s47g6w2x0spndwwl80000gn/T/ipykernel_70180/3442344895.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"text\"] = df.apply(generate_prompt, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Transform Gemma prompt template (https://ai.google.dev/gemma/docs/formatting)\n",
    "# {\"text\": \"<bos><start_of_turn>user\\nWhat is the capital of France?<end_of_turn>\\n<start_of_turn>model\\nParis is the capital of France.<end_of_turn><eos>\"}\n",
    "\n",
    "def generate_prompt(row: pd.Series) -> str:\n",
    "    \"Format to Gemma's chat template\"\n",
    "    return \"\"\"<bos><start_of_turn>user\n",
    "## Instructions\n",
    "{}\n",
    "## User\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn>\"\"\".format(row[\"system_prompt\"], row[\"question\"], row[\"response\"])\n",
    "\n",
    "\n",
    "df[\"text\"] = df.apply(generate_prompt, axis=1)\n",
    "print(df[\"text\"].iloc[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"text\":\"<bos><start_of_turn>user\\n## Instructions\\nYou are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\\n\\n- Documentation Requirements\\n- Operational Requirements\\n- Restrictions and Prohibited Items \\n- Additional Requirements\\n- Special Handling Requirements\\n- Contact Information\\n\\nKey behaviors:\\n\\nOffer current information on CHERRY's policies and maritime regulations\\nUse clear language, adapting to the user's expertise level\\nEmphasize safety, compliance, and environmental protection\\nProvide practical advice and clarify misunderstandings\\nReference the CHERRY Compliance document for detailed information\\nFor country-specific queries, use the relevant country's section in the document\\n\\nWhen responding:\\n\\nBe concise yet thorough\\nIf information is unavailable or unknown, clearly state \\\"No Information Available\\\"\\nHighlight recent regulatory changes when relevant\\nEnd each response with '–CHERRY_Compliance'\\n## User\\nWhat documentation is necessary for importing animal products into South Africa?<end_of_turn>\\n<start_of_turn>model\\nAnimal products require veterinary health certificates.<end_of_turn>\"}\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to train and valid \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "split_ix = int(len(df) * 0.9)\n",
    "# shuffle data\n",
    "data = df.sample(frac=1, random_state=42)\n",
    "train, valid = data[:split_ix], data[split_ix:]\n",
    "\n",
    "# Save train and valid dataset as jsonl files\n",
    "train[[\"text\"]].to_json(\"data/train.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "valid[[\"text\"]].to_json(\"data/valid.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "!head -n 1 data/train.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: lora.py [-h] [--model MODEL] [--train] [--data DATA]\n",
      "               [--lora-layers LORA_LAYERS] [--batch-size BATCH_SIZE]\n",
      "               [--iters ITERS] [--val-batches VAL_BATCHES]\n",
      "               [--learning-rate LEARNING_RATE]\n",
      "               [--steps-per-report STEPS_PER_REPORT]\n",
      "               [--steps-per-eval STEPS_PER_EVAL]\n",
      "               [--resume-adapter-file RESUME_ADAPTER_FILE]\n",
      "               [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\n",
      "               [--test] [--test-batches TEST_BATCHES]\n",
      "               [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\n",
      "               [--grad-checkpoint] [--seed SEED] [--use-dora]\n",
      "\n",
      "LoRA or QLoRA finetuning.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --train               Do training\n",
      "  --data DATA           Directory with {train, valid, test}.jsonl files\n",
      "  --lora-layers LORA_LAYERS\n",
      "                        Number of layers to fine-tune. Default is 16, use -1\n",
      "                        for all.\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Minibatch size.\n",
      "  --iters ITERS         Iterations to train for.\n",
      "  --val-batches VAL_BATCHES\n",
      "                        Number of validation batches, -1 uses the entire\n",
      "                        validation set.\n",
      "  --learning-rate LEARNING_RATE\n",
      "                        Adam learning rate.\n",
      "  --steps-per-report STEPS_PER_REPORT\n",
      "                        Number of training steps between loss reporting.\n",
      "  --steps-per-eval STEPS_PER_EVAL\n",
      "                        Number of training steps between validations.\n",
      "  --resume-adapter-file RESUME_ADAPTER_FILE\n",
      "                        Load path to resume training with the given adapters.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Save/load path for the adapters.\n",
      "  --save-every SAVE_EVERY\n",
      "                        Save the model every N iterations.\n",
      "  --test                Evaluate on the test set after training\n",
      "  --test-batches TEST_BATCHES\n",
      "                        Number of test set batches, -1 uses the entire test\n",
      "                        set.\n",
      "  --max-seq-length MAX_SEQ_LENGTH\n",
      "                        Maximum sequence length.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        A YAML configuration file with the training options\n",
      "  --grad-checkpoint     Use gradient checkpointing to reduce memory use.\n",
      "  --seed SEED           The PRNG seed\n",
      "  --use-dora            Use DoRA to finetune.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 9 files: 100%|███████████████████████| 9/9 [00:00<00:00, 108162.57it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.007% (1.966M/27227.128M)\n",
      "Starting training..., iters: 200\n",
      "Iter 1: Val loss 3.598, Val took 206.026s\n",
      "Iter 10: Val loss 2.665, Val took 238.179s\n",
      "Iter 10: Train loss 3.075, Learning Rate 1.000e-05, It/sec 0.612, Tokens/sec 582.463, Trained Tokens 9513, Peak mem 31.318 GB\n",
      "Iter 20: Val loss 1.794, Val took 245.594s\n",
      "Iter 20: Train loss 2.168, Learning Rate 1.000e-05, It/sec 0.670, Tokens/sec 643.753, Trained Tokens 19116, Peak mem 32.516 GB\n",
      "Iter 30: Val loss 0.849, Val took 215.245s\n",
      "Iter 30: Train loss 1.217, Learning Rate 1.000e-05, It/sec 0.664, Tokens/sec 633.840, Trained Tokens 28657, Peak mem 35.784 GB\n",
      "Iter 40: Val loss 0.504, Val took 221.943s\n",
      "Iter 40: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.607, Tokens/sec 582.921, Trained Tokens 38253, Peak mem 35.784 GB\n",
      "Iter 50: Val loss 0.374, Val took 200.716s\n",
      "Iter 50: Train loss 0.454, Learning Rate 1.000e-05, It/sec 0.655, Tokens/sec 619.029, Trained Tokens 47704, Peak mem 35.784 GB\n",
      "Iter 60: Val loss 0.365, Val took 239.240s\n",
      "Iter 60: Train loss 0.435, Learning Rate 1.000e-05, It/sec 0.650, Tokens/sec 632.258, Trained Tokens 57433, Peak mem 35.784 GB\n",
      "Iter 70: Val loss 0.359, Val took 217.690s\n",
      "Iter 70: Train loss 0.338, Learning Rate 1.000e-05, It/sec 0.616, Tokens/sec 564.165, Trained Tokens 66593, Peak mem 35.784 GB\n",
      "Iter 80: Val loss 0.366, Val took 268.015s\n",
      "Iter 80: Train loss 0.346, Learning Rate 1.000e-05, It/sec 0.514, Tokens/sec 488.975, Trained Tokens 76102, Peak mem 35.784 GB\n",
      "Iter 90: Val loss 0.356, Val took 219.036s\n",
      "Iter 90: Train loss 0.349, Learning Rate 1.000e-05, It/sec 0.619, Tokens/sec 588.773, Trained Tokens 85608, Peak mem 35.784 GB\n",
      "Iter 100: Val loss 0.339, Val took 207.842s\n",
      "Iter 100: Train loss 0.328, Learning Rate 1.000e-05, It/sec 0.518, Tokens/sec 478.492, Trained Tokens 94842, Peak mem 35.784 GB\n",
      "Iter 100: Saved adapter weights to checkpoints/adapters/adapters.safetensors and checkpoints/adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Val loss 0.338, Val took 207.412s\n",
      "Iter 110: Train loss 0.304, Learning Rate 1.000e-05, It/sec 0.469, Tokens/sec 444.515, Trained Tokens 104311, Peak mem 35.784 GB\n",
      "Iter 120: Val loss 0.328, Val took 256.130s\n",
      "Iter 120: Train loss 0.312, Learning Rate 1.000e-05, It/sec 0.563, Tokens/sec 517.292, Trained Tokens 113495, Peak mem 35.784 GB\n",
      "Iter 130: Val loss 0.312, Val took 206.791s\n",
      "Iter 130: Train loss 0.289, Learning Rate 1.000e-05, It/sec 0.667, Tokens/sec 626.452, Trained Tokens 122890, Peak mem 35.784 GB\n",
      "Iter 140: Val loss 0.310, Val took 214.118s\n",
      "Iter 140: Train loss 0.307, Learning Rate 1.000e-05, It/sec 0.620, Tokens/sec 581.078, Trained Tokens 132255, Peak mem 35.784 GB\n",
      "Iter 150: Val loss 0.325, Val took 217.813s\n",
      "Iter 150: Train loss 0.352, Learning Rate 1.000e-05, It/sec 0.574, Tokens/sec 548.978, Trained Tokens 141812, Peak mem 35.784 GB\n",
      "Iter 160: Val loss 0.320, Val took 216.286s\n",
      "Iter 160: Train loss 0.316, Learning Rate 1.000e-05, It/sec 0.527, Tokens/sec 497.487, Trained Tokens 151247, Peak mem 35.784 GB\n",
      "Iter 170: Val loss 0.302, Val took 200.699s\n",
      "Iter 170: Train loss 0.345, Learning Rate 1.000e-05, It/sec 0.530, Tokens/sec 514.211, Trained Tokens 160958, Peak mem 35.784 GB\n",
      "Iter 180: Val loss 0.318, Val took 208.667s\n",
      "Iter 180: Train loss 0.290, Learning Rate 1.000e-05, It/sec 0.618, Tokens/sec 583.581, Trained Tokens 170396, Peak mem 35.784 GB\n",
      "Iter 190: Val loss 0.316, Val took 215.349s\n",
      "Iter 190: Train loss 0.327, Learning Rate 1.000e-05, It/sec 0.626, Tokens/sec 592.993, Trained Tokens 179876, Peak mem 35.784 GB\n",
      "Iter 200: Val loss 0.338, Val took 264.522s\n",
      "Iter 200: Train loss 0.324, Learning Rate 1.000e-05, It/sec 0.503, Tokens/sec 478.396, Trained Tokens 189394, Peak mem 35.784 GB\n",
      "Iter 200: Saved adapter weights to checkpoints/adapters/adapters.safetensors and checkpoints/adapters/0000200_adapters.safetensors.\n",
      "Saved final adapter weights to checkpoints/adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.lora \\\n",
    "    --model mlx-community/gemma-2-27b-it-4bit \\\n",
    "    --train \\\n",
    "    --data data \\\n",
    "    --iters 300 \\\n",
    "    --batch-size 4 \\\n",
    "    --learning-rate 1e-5 \\\n",
    "    --steps-per-report 10 \\\n",
    "    --steps-per-eval 10 \\\n",
    "    --adapter-path checkpoints/adapters \\\n",
    "    # --resume-adapter-file checkpoints/600_adapters.npz \\\n",
    "    --save-every 10 \\\n",
    "    --max-seq-length 2048 \\\n",
    "    --seed 42 \\\n",
    "    --lora-layers 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m mlx_lm.lora \\\n",
    "#     --model mlx-community/gemma-2-27b-it-4bit \\\n",
    "#     --train \\\n",
    "#     --data data \\\n",
    "#     --iters 600 \\\n",
    "#     --batch-size 4 \\\n",
    "#     --learning-rate 1e-5 \\\n",
    "#     --steps-per-report 10 \\\n",
    "#     --steps-per-eval 10 \\\n",
    "#     --adapter-path checkpoints/adapters \\\n",
    "#     --resume-adapter-file checkpoints/adapters/0000100_adapters.safetensors \\\n",
    "#     --save-every 10 \\\n",
    "#     --max-seq-length 2048 \\\n",
    "#     --seed 42 \\\n",
    "#     --lora-layers 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
      "\n",
      "- Documentation Requirements\n",
      "- Operational Requirements\n",
      "- Restrictions and Prohibited Items \n",
      "- Additional Requirements\n",
      "- Special Handling Requirements\n",
      "- Contact Information\n",
      "\n",
      "Key behaviors:\n",
      "\n",
      "Offer current information on CHERRY's policies and maritime regulations\n",
      "Use clear language, adapting to the user's expertise level\n",
      "Emphasize safety, compliance, and environmental protection\n",
      "Provide practical advice and clarify misunderstandings\n",
      "Reference the CHERRY Compliance document for detailed information\n",
      "For country-specific queries, use the relevant country's section in the document\n",
      "\n",
      "When responding:\n",
      "\n",
      "Be concise yet thorough\n",
      "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
      "Highlight recent regulatory changes when relevant\n",
      "End each response with '–CHERRY_Compliance'\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "\n",
    "system_prompt = df[\"system_prompt\"].unique()[-1]\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
      "\n",
      "- Documentation Requirements\n",
      "- Operational Requirements\n",
      "- Restrictions and Prohibited Items \n",
      "- Additional Requirements\n",
      "- Special Handling Requirements\n",
      "- Contact Information\n",
      "\n",
      "Key behaviors:\n",
      "\n",
      "Offer current information on CHERRY's policies and maritime regulations\n",
      "Use clear language, adapting to the user's expertise level\n",
      "Emphasize safety, compliance, and environmental protection\n",
      "Provide practical advice and clarify misunderstandings\n",
      "Reference the CHERRY Compliance document for detailed information\n",
      "For country-specific queries, use the relevant country's section in the document\n",
      "\n",
      "When responding:\n",
      "\n",
      "Be concise yet thorough\n",
      "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
      "Highlight recent regulatory changes when relevant\n",
      "End each response with '–CHERRY_Compliance'\n",
      "## User\n",
      "What are the normal working hours for customs at Dar es Salaam Port? Let me know as detail as possible.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the normal working hours for customs at Dar es Salaam Port? Let me know as detail as possible.\"\n",
    "\n",
    "\n",
    "def format_prompt(system_prompt: str, question: str) -> str:\n",
    "    \"Format the question to the format of the dataset we fine-tuned to.\"\n",
    "    return \"\"\"<bos><start_of_turn>user\n",
    "## Instructions\n",
    "{}\n",
    "## User\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\".format(\n",
    "        system_prompt, question\n",
    "    )\n",
    "\n",
    "\n",
    "print(format_prompt(system_prompt, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8a3895f7ae4cf28fcb0d4fe761a4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fine-tuned model with LoRA weights\n",
    "model_lora, tokenizer = load(\n",
    "    \"mlx-community/gemma-2-27b-it-4bit\",\n",
    "    adapter_path=\"./checkpoints/adapters\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
      "\n",
      "- Documentation Requirements\n",
      "- Operational Requirements\n",
      "- Restrictions and Prohibited Items \n",
      "- Additional Requirements\n",
      "- Special Handling Requirements\n",
      "- Contact Information\n",
      "\n",
      "Key behaviors:\n",
      "\n",
      "Offer current information on CHERRY's policies and maritime regulations\n",
      "Use clear language, adapting to the user's expertise level\n",
      "Emphasize safety, compliance, and environmental protection\n",
      "Provide practical advice and clarify misunderstandings\n",
      "Reference the CHERRY Compliance document for detailed information\n",
      "For country-specific queries, use the relevant country's section in the document\n",
      "\n",
      "When responding:\n",
      "\n",
      "Be concise yet thorough\n",
      "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
      "Highlight recent regulatory changes when relevant\n",
      "End each response with '–CHERRY_Compliance'\n",
      "## User\n",
      "What are the normal working hours for customs at Dar es Salaam Port? Let me know as detail as possible.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Customs at Dar es Salaam Port operate from 08:00 to 17:00 on weekdays, excluding public holidays.<end_of_turn>\n",
      "==========\n",
      "Prompt: 208 tokens, 77.807 tokens-per-sec\n",
      "Generation: 30 tokens, 12.927 tokens-per-sec\n",
      "Peak memory: 29.180 GB\n"
     ]
    }
   ],
   "source": [
    "response = generate(\n",
    "    model_lora,\n",
    "    tokenizer,\n",
    "    prompt=format_prompt(system_prompt, question),\n",
    "    verbose=True,\n",
    "    temp=0.5,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <bos><start_of_turn>user\n",
      "## Instructions\n",
      "You are CHERRY_Compliance, a virtual expert for CHERRY Shipping Company. Provide accurate information on:\n",
      "\n",
      "- Documentation Requirements\n",
      "- Operational Requirements\n",
      "- Restrictions and Prohibited Items \n",
      "- Additional Requirements\n",
      "- Special Handling Requirements\n",
      "- Contact Information\n",
      "\n",
      "Key behaviors:\n",
      "\n",
      "Offer current information on CHERRY's policies and maritime regulations\n",
      "Use clear language, adapting to the user's expertise level\n",
      "Emphasize safety, compliance, and environmental protection\n",
      "Provide practical advice and clarify misunderstandings\n",
      "Reference the CHERRY Compliance document for detailed information\n",
      "For country-specific queries, use the relevant country's section in the document\n",
      "\n",
      "When responding:\n",
      "\n",
      "Be concise yet thorough\n",
      "If information is unavailable or unknown, clearly state \"No Information Available\"\n",
      "Highlight recent regulatory changes when relevant\n",
      "End each response with '–CHERRY_Compliance'\n",
      "## User\n",
      "What are the normal working hours for customs at Dar es Salaam Port? Let me know as detail as possible.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "The customs office at Dar es Salaam Port typically operates from 8:00 AM to 5:00 PM, Monday to Friday. However, it's best to confirm the exact hours with the port authorities or a local customs broker, as there may be variations depending on the day and workload.\n",
      "\n",
      "For the most up-to-date information, I recommend contacting the Tanzania Revenue Authority (TRA) directly.\n",
      "\n",
      "–CHERRY_Compliance \n",
      "<end_of_turn>\n",
      "\n",
      "==========\n",
      "Prompt: 208 tokens, 42.648 tokens-per-sec\n",
      "Generation: 96 tokens, 14.790 tokens-per-sec\n",
      "Peak memory: 29.194 GB\n"
     ]
    }
   ],
   "source": [
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=format_prompt(system_prompt, question),\n",
    "    verbose=True,\n",
    "    temp=0.5,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|████████████████████████| 9/9 [00:00<00:00, 26696.42it/s]\n",
      "==========\n",
      "Prompt: <bos><start_of_turn>user\n",
      "What are the normal working hours for customs at Dar es Salaam Port?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "I cannot provide specific real-time information like working hours for government offices. Working hours are subject to change and are best obtained directly from the source.<end_of_turn>\n",
      "==========\n",
      "Prompt: 24 tokens, 12.770 tokens-per-sec\n",
      "Generation: 32 tokens, 13.584 tokens-per-sec\n",
      "Peak memory: 14.642 GB\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate \\\n",
    "    --model mlx-community/gemma-2-27b-it-4bit \\\n",
    "    --adapter-path checkpoints/adapters \\\n",
    "    --prompt \"What are the normal working hours for customs at Dar es Salaam Port?\" \\\n",
    "    --max-tokens 256 \\\n",
    "    --temp 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: generate.py [-h] [--model MODEL] [--adapter-path ADAPTER_PATH]\n",
      "                   [--trust-remote-code] [--eos-token EOS_TOKEN]\n",
      "                   [--prompt PROMPT] [--max-tokens MAX_TOKENS] [--temp TEMP]\n",
      "                   [--top-p TOP_P] [--seed SEED] [--ignore-chat-template]\n",
      "                   [--use-default-chat-template] [--colorize]\n",
      "                   [--cache-limit-gb CACHE_LIMIT_GB]\n",
      "\n",
      "LLM inference script\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\n",
      "                        repo.\n",
      "  --adapter-path ADAPTER_PATH\n",
      "                        Optional path for the trained adapter weights and\n",
      "                        config.\n",
      "  --trust-remote-code   Enable trusting remote code for tokenizer\n",
      "  --eos-token EOS_TOKEN\n",
      "                        End of sequence token for tokenizer\n",
      "  --prompt PROMPT       Message to be processed by the model\n",
      "  --max-tokens MAX_TOKENS, -m MAX_TOKENS\n",
      "                        Maximum number of tokens to generate\n",
      "  --temp TEMP           Sampling temperature\n",
      "  --top-p TOP_P         Sampling top-p\n",
      "  --seed SEED           PRNG seed\n",
      "  --ignore-chat-template\n",
      "                        Use the raw prompt without the tokenizer's chat\n",
      "                        template.\n",
      "  --use-default-chat-template\n",
      "                        Use the default chat template\n",
      "  --colorize            Colorize output based on T[0] probability\n",
      "  --cache-limit-gb CACHE_LIMIT_GB\n",
      "                        Set the MLX cache limit in GB\n"
     ]
    }
   ],
   "source": [
    "!python -m mlx_lm.generate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
